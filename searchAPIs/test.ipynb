{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'job_descriptions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the Kaggle dataset\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjob_descriptions.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Preprocess the text data\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# (Tokenization, lowercasing, stopword removal, etc.)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract features (TF-IDF)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/learnai/lib/python3.11/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/learnai/lib/python3.11/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/learnai/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/learnai/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/learnai/lib/python3.11/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'job_descriptions.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Kaggle dataset\n",
    "data = pd.read_csv('job_descriptions.csv')\n",
    "\n",
    "# Preprocess the text data\n",
    "# (Tokenization, lowercasing, stopword removal, etc.)\n",
    "\n",
    "# Extract features (TF-IDF)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(data['job_description'])\n",
    "y = data['skill_label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting from job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is working...\n",
      "[]\n",
      "Query:  tutorial\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GalalEwida/LLM-BERT-Model-Based-Skills-Extraction-from-jobdescription\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"GalalEwida/LLM-BERT-Model-Based-Skills-Extraction-from-jobdescription\")\n",
    "\n",
    "job_description = \"\"\"Company Description\n",
    "\n",
    "The Company: Prime Robotics is a leading global automation company serving the Logistics, Manufacturing and E-Commerce industries. We develop cutting-edge solutions that are innovative, productive, and scalable. Our mission is to accelerate supply chain logistics via best -in-class robotic solutions. We are in a startup mode, which means we are looking for someone with an entrepreneurial spirit who can tackle obstacles and not allow roadblocks, large or small to get in the way or slow down our processes. Is that you?\n",
    "\n",
    "Prime Robotics is proud to be an EEOE, M/F/D/V, and we are committed to diversity both in practice and spirit at the corporate level.\n",
    "\n",
    "Prime Robotics participates in E-Verify. E-Verify is an Internet-based system that compares information from an employee's Form I-9, Employment Eligibility Verification, to data from U.S. Department of Homeland Security and Social Security Administration records to confirm employment eligibility.\n",
    "\n",
    "Job Description\n",
    "\n",
    "You in The Role and on The Team: In this role you are SLAM Engineer with a background in Engineering and Robotics. Your duties may range from working on LiDAR-based SLAM, evaluating different sensors, visualizing data in order to make decisions on how to improve the system, and sensor fusion between LiDAR, and computer vision. Your experience includes having a deep understanding of the software and mechanics of Robotics including, Drivers, Camera’s, Lidars, sensors, autonomous vehicles, robots that move.\n",
    "\n",
    "A Day in the Life\n",
    "• Design, develop, implement, and optimize SLAM algorithms for computer vision and robotics systems\n",
    "• Collaborate with cross-functional teams to integrate SLAM technology into our products.\n",
    "• Work with cameras, IMUs, GPS, and other sensors to generate 3D maps and trajectories.\n",
    "• Develop, test, and optimize SLAM backends using G2O or GTSAM\n",
    "• Create and maintain code documentation, unit tests, and system test suites\n",
    "• Explore new vision-based sensors and technologies\n",
    "• Design custom architecture for vision-based autonomy\n",
    "• Collaborate with cross-functional teams to integrate models and algorithms technology into our products\n",
    "• Collaborate with cross-functional teams to integrate SLAM technology into our products.\n",
    "• Work with cameras, IMUs, GPS, and other sensors to generate 3D maps and trajectories.\n",
    "• Develop, test, and optimize SLAM backends using G2O or GTSAM\n",
    "• Create and maintain code documentation, unit tests, and systems test suites\n",
    "\n",
    "Qualifications\n",
    "\n",
    "Key success factors\n",
    "• 3+ years of experience, ideally in a robotics or autonomous systems field\n",
    "• Hands on experience integrating sensors and algorithms on embedded systems\n",
    "• Proficient in C++, Python, ROS2, and/or other robotics programming languages\n",
    "• Strong background in SLAM, 3D reconstruction, Structure-from-Motion, Visual Inertial Odometry, and/or Bundle Adjustment\n",
    "• Experience with SLAM backends such as G2O or GTSAM\n",
    "• Strong foundations in multi-view geometry\n",
    "• Experience in camera calibration\n",
    "• Expert in complex rotations and frame transformations\n",
    "• Strong problem-solving skills and ability to work in a fast-paced environment\n",
    "• Strong verbal and written communication skills\n",
    "• Experience with sensor fusion techniques to enhance positioning accuracy using data from various sensors like IMUs, LiDAR, and cameras.\n",
    "• Experience developing scalable training pipelines in the cloud\n",
    "• Experience with sensor fusion techniques to enhance positioning accuracy using data from various sensors like IMUs, LiDAR, and cameras\n",
    "• Enthusiasm for the field of robotics and troubleshooting complex systems\n",
    "• Bachelor’s Degree in Robotics, Mechanical or Electrical Engineering or relevant degree\"\"\"\n",
    "\n",
    "# Tokenize the job description\n",
    "tokens = tokenizer(job_description, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Pass the tokens through the model\n",
    "outputs = model(**tokens)\n",
    "\n",
    "# Extract the predicted labels\n",
    "predicted_labels = outputs.logits.argmax(-1)\n",
    "\n",
    "# Print a message to indicate that the model is working\n",
    "print(\"Model is working...\")\n",
    "label_names = model.config.id2label\n",
    "\n",
    "# Extract skills based on predicted labels\n",
    "extracted_skills = []\n",
    "for token, label_id in zip(tokens, predicted_labels[0]):\n",
    "    label = label_names[1]\n",
    "    if label == \"SKILL\":\n",
    "        extracted_skills.append(token)\n",
    "\n",
    "# Print the extracted skills\n",
    "print(extracted_skills)\n",
    "for skill in extracted_skills:\n",
    "    print(skill)\n",
    "\n",
    "# Construct query from skills\n",
    "query = \" \".join(extracted_skills) + \" tutorial\"  # Add \"tutorial\" to refine the search\n",
    "\n",
    "# Example query\n",
    "print(\"Query:\", query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch articles using Google Custom Search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_google_search_results(query, api_key, cx):\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cx}\"\n",
    "    response = requests.get(url)\n",
    "    results = response.json()\n",
    "    return [item['link'] for item in results.get('items', [])]\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    return ' '.join([para.get_text() for para in paragraphs])\n",
    "\n",
    "api_key = ''\n",
    "cx = ''\n",
    "query = 'Resouces on becoming a Machine Learning engineer'\n",
    "\n",
    "google_urls = fetch_google_search_results(query, api_key, cx)\n",
    "google_articles = [fetch_article_content(url) for url in google_urls if url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key topics in the job description:\n",
      "Company Description\n",
      "Company\n",
      "Prime Robotics\n",
      "automation company\n",
      "Logistics\n",
      "Manufacturing\n",
      "E-Commerce industries\n",
      "solutions\n",
      "mission\n",
      "supply chain logistics\n",
      "-in-class\n",
      "solutions\n",
      "startup mode\n",
      "someone\n",
      "spirit\n",
      "obstacles\n",
      "roadblocks\n",
      "way\n",
      "processes\n",
      "Prime Robotics\n",
      "EEOE\n",
      "M/F/D/V\n",
      "practice\n",
      "spirit\n",
      "level\n",
      "Prime Robotics\n",
      "E-Verify\n",
      "E-Verify\n",
      "system\n",
      "information\n",
      "employee\n",
      "Form I-9\n",
      "Employment Eligibility Verification\n",
      "data\n",
      "U.S. Department\n",
      "Homeland Security\n",
      "Social Security Administration records\n",
      "employment eligibility\n",
      "Job Description\n",
      "Role\n",
      "Team\n",
      "role\n",
      "SLAM Engineer\n",
      "background\n",
      "Engineering\n",
      "Robotics\n",
      "duties\n",
      "SLAM\n",
      "sensors\n",
      "data\n",
      "order\n",
      "decisions\n",
      "system\n",
      "sensor fusion\n",
      "LiDAR\n",
      "computer vision\n",
      "experience\n",
      "understanding\n",
      "software\n",
      "mechanics\n",
      "Robotics\n",
      "Drivers\n",
      "Camera ’ s\n",
      "Lidars\n",
      "sensors\n",
      "vehicles\n",
      "robots\n",
      "move\n",
      "Day\n",
      "Life • Design\n",
      "develop\n",
      "implement\n",
      "SLAM algorithms\n",
      "computer vision\n",
      "robotics systems\n",
      "Collaborate\n",
      "teams\n",
      "SLAM technology\n",
      "products\n",
      "Work\n",
      "cameras\n",
      "IMUs\n",
      "GPS\n",
      "sensors\n",
      "maps\n",
      "trajectories\n",
      "• Develop\n",
      "test\n",
      "SLAM backends\n",
      "G2O\n",
      "GTSAM • Create\n",
      "code documentation\n",
      "unit tests\n",
      "system test\n",
      "• Explore\n",
      "sensors\n",
      "technologies\n",
      "Design custom architecture\n",
      "autonomy\n",
      "Collaborate\n",
      "teams\n",
      "models\n",
      "technology\n",
      "products\n",
      "Collaborate\n",
      "teams\n",
      "SLAM technology\n",
      "products\n",
      "Work\n",
      "cameras\n",
      "IMUs\n",
      "GPS\n",
      "sensors\n",
      "maps\n",
      "trajectories\n",
      "• Develop\n",
      "test\n",
      "SLAM backends\n",
      "G2O\n",
      "GTSAM • Create\n",
      "code documentation\n",
      "unit tests\n",
      "systems\n",
      "suites Qualifications Key success factors\n",
      "years\n",
      "experience\n",
      "robotics\n",
      "systems field • Hands\n",
      "experience integrating sensors\n",
      "algorithms\n",
      "systems\n",
      "Proficient\n",
      "C++\n",
      "Python\n",
      "ROS2\n",
      "robotics\n",
      "languages\n",
      "background\n",
      "SLAM\n",
      "reconstruction\n",
      "Structure-from-Motion\n",
      "Visual Inertial Odometry\n",
      "and/or Bundle Adjustment • Experience\n",
      "SLAM backends\n",
      "G2O\n",
      "GTSAM • Strong foundations\n",
      "geometry • Experience\n",
      "camera calibration • Expert\n",
      "rotations\n",
      "transformations\n",
      "skills\n",
      "ability\n",
      "environment • Strong verbal\n",
      "communication skills\n",
      "Experience\n",
      "fusion techniques\n",
      "accuracy\n",
      "data\n",
      "sensors\n",
      "IMUs\n",
      "LiDAR\n",
      "cameras\n",
      "Experience\n",
      "training pipelines\n",
      "cloud • Experience\n",
      "fusion techniques\n",
      "accuracy\n",
      "data\n",
      "sensors\n",
      "IMUs\n",
      "LiDAR\n",
      "cameras\n",
      "Enthusiasm\n",
      "field\n",
      "robotics\n",
      "systems • Bachelor ’\n",
      "Degree\n",
      "Robotics\n",
      "Mechanical\n",
      "Electrical Engineering\n",
      "degree\n",
      "Number of articles: 5\n",
      "Articles: [('AFWERX Spark empowered innovation at the operational edge, and AFWERX Prime accelerated emerging technology markets using military missions and resources. The\\xa0...', 'https://afwerx.com/wp-content/uploads/Program_Overview_CLEARED-AFRL-2022-5908_web-1.pdf'), (\"It's easy to see why some oil & gas companies rank higher than Tesla on “Environmental Impact.” “The most striking feature of the [ESG rating] system is how\\xa0...\", 'https://www.tesla.com/ns_videos/2021-tesla-impact-report.pdf'), ('Artificial intelligence is getting ready for business, but are businesses ready for AI? Tera Allas, Jacques Bughin, Michael Chui, Peter Dahlström, Eric Hazan,\\xa0...', 'https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/Analytics%20comes%20of%20age/Analytics-comes-of-age.ashx'), ('Apr 9, 2021 ... acquisition categories - or ACATs and three business categories or BCATs. ACAT I: Eventual expenditure of more than $480M in RDT&E, or more than\\xa0...', 'https://www.af.mil/Portals/1/documents/2021SAF/04_Apr/FY19_FY20_Dept_of_the_Air_Force_Acquisition_Biennial_Report_final.pdf'), ('However, embracing IoT adoption emerges as the decisive step towards comprehensive business management, ushering in a multitude of transformative outcomes. IoT\\xa0...', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10574902/')]\n",
      "Number of articles: 5\n",
      "Articles: [('AFWERX Spark empowered innovation at the operational edge, and AFWERX Prime accelerated emerging technology markets using military missions and resources. The\\xa0...', 'https://afwerx.com/wp-content/uploads/Program_Overview_CLEARED-AFRL-2022-5908_web-1.pdf'), (\"It's easy to see why some oil & gas companies rank higher than Tesla on “Environmental Impact.” “The most striking feature of the [ESG rating] system is how\\xa0...\", 'https://www.tesla.com/ns_videos/2021-tesla-impact-report.pdf'), ('Artificial intelligence is getting ready for business, but are businesses ready for AI? Tera Allas, Jacques Bughin, Michael Chui, Peter Dahlström, Eric Hazan,\\xa0...', 'https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/Analytics%20comes%20of%20age/Analytics-comes-of-age.ashx'), ('Apr 9, 2021 ... acquisition categories - or ACATs and three business categories or BCATs. ACAT I: Eventual expenditure of more than $480M in RDT&E, or more than\\xa0...', 'https://www.af.mil/Portals/1/documents/2021SAF/04_Apr/FY19_FY20_Dept_of_the_Air_Force_Acquisition_Biennial_Report_final.pdf'), ('However, embracing IoT adoption emerges as the decisive step towards comprehensive business management, ushering in a multitude of transformative outcomes. IoT\\xa0...', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10574902/')]\n",
      "Number of articles: 5\n",
      "Articles: [('AFWERX Spark empowered innovation at the operational edge, and AFWERX Prime accelerated emerging technology markets using military missions and resources. The\\xa0...', 'https://afwerx.com/wp-content/uploads/Program_Overview_CLEARED-AFRL-2022-5908_web-1.pdf'), (\"It's easy to see why some oil & gas companies rank higher than Tesla on “Environmental Impact.” “The most striking feature of the [ESG rating] system is how\\xa0...\", 'https://www.tesla.com/ns_videos/2021-tesla-impact-report.pdf'), ('Artificial intelligence is getting ready for business, but are businesses ready for AI? Tera Allas, Jacques Bughin, Michael Chui, Peter Dahlström, Eric Hazan,\\xa0...', 'https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/Analytics%20comes%20of%20age/Analytics-comes-of-age.ashx'), ('Apr 9, 2021 ... acquisition categories - or ACATs and three business categories or BCATs. ACAT I: Eventual expenditure of more than $480M in RDT&E, or more than\\xa0...', 'https://www.af.mil/Portals/1/documents/2021SAF/04_Apr/FY19_FY20_Dept_of_the_Air_Force_Acquisition_Biennial_Report_final.pdf'), ('However, embracing IoT adoption emerges as the decisive step towards comprehensive business management, ushering in a multitude of transformative outcomes. IoT\\xa0...', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10574902/')]\n",
      "Number of articles: 5\n",
      "Articles: [('AFWERX Spark empowered innovation at the operational edge, and AFWERX Prime accelerated emerging technology markets using military missions and resources. The\\xa0...', 'https://afwerx.com/wp-content/uploads/Program_Overview_CLEARED-AFRL-2022-5908_web-1.pdf'), (\"It's easy to see why some oil & gas companies rank higher than Tesla on “Environmental Impact.” “The most striking feature of the [ESG rating] system is how\\xa0...\", 'https://www.tesla.com/ns_videos/2021-tesla-impact-report.pdf'), ('Artificial intelligence is getting ready for business, but are businesses ready for AI? Tera Allas, Jacques Bughin, Michael Chui, Peter Dahlström, Eric Hazan,\\xa0...', 'https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/Analytics%20comes%20of%20age/Analytics-comes-of-age.ashx'), ('Apr 9, 2021 ... acquisition categories - or ACATs and three business categories or BCATs. ACAT I: Eventual expenditure of more than $480M in RDT&E, or more than\\xa0...', 'https://www.af.mil/Portals/1/documents/2021SAF/04_Apr/FY19_FY20_Dept_of_the_Air_Force_Acquisition_Biennial_Report_final.pdf'), ('However, embracing IoT adoption emerges as the decisive step towards comprehensive business management, ushering in a multitude of transformative outcomes. IoT\\xa0...', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10574902/')]\n",
      "Number of articles: 5\n",
      "Articles: [('AFWERX Spark empowered innovation at the operational edge, and AFWERX Prime accelerated emerging technology markets using military missions and resources. The\\xa0...', 'https://afwerx.com/wp-content/uploads/Program_Overview_CLEARED-AFRL-2022-5908_web-1.pdf'), (\"It's easy to see why some oil & gas companies rank higher than Tesla on “Environmental Impact.” “The most striking feature of the [ESG rating] system is how\\xa0...\", 'https://www.tesla.com/ns_videos/2021-tesla-impact-report.pdf'), ('Artificial intelligence is getting ready for business, but are businesses ready for AI? Tera Allas, Jacques Bughin, Michael Chui, Peter Dahlström, Eric Hazan,\\xa0...', 'https://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/Analytics%20comes%20of%20age/Analytics-comes-of-age.ashx'), ('Apr 9, 2021 ... acquisition categories - or ACATs and three business categories or BCATs. ACAT I: Eventual expenditure of more than $480M in RDT&E, or more than\\xa0...', 'https://www.af.mil/Portals/1/documents/2021SAF/04_Apr/FY19_FY20_Dept_of_the_Air_Force_Acquisition_Biennial_Report_final.pdf'), ('However, embracing IoT adoption emerges as the decisive step towards comprehensive business management, ushering in a multitude of transformative outcomes. IoT\\xa0...', 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10574902/')]\n",
      "Number of Similarity Scores: 0\n",
      "Google Articles Relevance:\n",
      "URL: https://afwerx.com/wp-content/uploads/Program_Overview_CLEARED-AFRL-2022-5908_web-1.pdf\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 130\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (sentence, link) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(google_articles):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlink\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mgoogle_similarity_scores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Initialize components\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "# Example job description\n",
    "job_description = \"\"\"Company Description\n",
    "\n",
    "The Company: Prime Robotics is a leading global automation company serving the Logistics, Manufacturing and E-Commerce industries. We develop cutting-edge solutions that are innovative, productive, and scalable. Our mission is to accelerate supply chain logistics via best -in-class robotic solutions. We are in a startup mode, which means we are looking for someone with an entrepreneurial spirit who can tackle obstacles and not allow roadblocks, large or small to get in the way or slow down our processes. Is that you?\n",
    "\n",
    "Prime Robotics is proud to be an EEOE, M/F/D/V, and we are committed to diversity both in practice and spirit at the corporate level.\n",
    "\n",
    "Prime Robotics participates in E-Verify. E-Verify is an Internet-based system that compares information from an employee's Form I-9, Employment Eligibility Verification, to data from U.S. Department of Homeland Security and Social Security Administration records to confirm employment eligibility.\n",
    "\n",
    "Job Description\n",
    "\n",
    "You in The Role and on The Team: In this role you are SLAM Engineer with a background in Engineering and Robotics. Your duties may range from working on LiDAR-based SLAM, evaluating different sensors, visualizing data in order to make decisions on how to improve the system, and sensor fusion between LiDAR, and computer vision. Your experience includes having a deep understanding of the software and mechanics of Robotics including, Drivers, Camera’s, Lidars, sensors, autonomous vehicles, robots that move.\n",
    "\n",
    "A Day in the Life\n",
    "• Design, develop, implement, and optimize SLAM algorithms for computer vision and robotics systems\n",
    "• Collaborate with cross-functional teams to integrate SLAM technology into our products.\n",
    "• Work with cameras, IMUs, GPS, and other sensors to generate 3D maps and trajectories.\n",
    "• Develop, test, and optimize SLAM backends using G2O or GTSAM\n",
    "• Create and maintain code documentation, unit tests, and system test suites\n",
    "• Explore new vision-based sensors and technologies\n",
    "• Design custom architecture for vision-based autonomy\n",
    "• Collaborate with cross-functional teams to integrate models and algorithms technology into our products\n",
    "• Collaborate with cross-functional teams to integrate SLAM technology into our products.\n",
    "• Work with cameras, IMUs, GPS, and other sensors to generate 3D maps and trajectories.\n",
    "• Develop, test, and optimize SLAM backends using G2O or GTSAM\n",
    "• Create and maintain code documentation, unit tests, and systems test suites\n",
    "\n",
    "Qualifications\n",
    "\n",
    "Key success factors\n",
    "• 3+ years of experience, ideally in a robotics or autonomous systems field\n",
    "• Hands on experience integrating sensors and algorithms on embedded systems\n",
    "• Proficient in C++, Python, ROS2, and/or other robotics programming languages\n",
    "• Strong background in SLAM, 3D reconstruction, Structure-from-Motion, Visual Inertial Odometry, and/or Bundle Adjustment\n",
    "• Experience with SLAM backends such as G2O or GTSAM\n",
    "• Strong foundations in multi-view geometry\n",
    "• Experience in camera calibration\n",
    "• Expert in complex rotations and frame transformations\n",
    "• Strong problem-solving skills and ability to work in a fast-paced environment\n",
    "• Strong verbal and written communication skills\n",
    "• Experience with sensor fusion techniques to enhance positioning accuracy using data from various sensors like IMUs, LiDAR, and cameras.\n",
    "• Experience developing scalable training pipelines in the cloud\n",
    "• Experience with sensor fusion techniques to enhance positioning accuracy using data from various sensors like IMUs, LiDAR, and cameras\n",
    "• Enthusiasm for the field of robotics and troubleshooting complex systems\n",
    "• Bachelor’s Degree in Robotics, Mechanical or Electrical Engineering or relevant degree\"\"\"\n",
    "# Tokenize the job description\n",
    "tokens = word_tokenize(job_description)\n",
    "\n",
    "# Part-of-speech tagging\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "# Identify noun phrases\n",
    "noun_phrases = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "\n",
    "# Filter and normalize skills\n",
    "skills = [word.lower() for word in noun_phrases if word.lower() in skill_list]\n",
    "\n",
    "# Function to fetch articles from Google search results\n",
    "def fetch_google_articles(query, api_key, cx):\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={api_key}&cx={cx}&num=5\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    articles = [(item['snippet'], item['link']) for item in data['items']]\n",
    "    return articles\n",
    "# Function to process articles\n",
    "def process_articles(articles, job_description):\n",
    "    relevant_sentences = []\n",
    "    similarity_scores = []\n",
    "    for snippet, link in articles:\n",
    "        sentences = sent_tokenize(snippet)\n",
    "        print(\"Number of articles:\", len(articles))\n",
    "        print(\"Articles:\", articles)\n",
    "        for sentence in sentences:\n",
    "            sentiment_score = sia.polarity_scores(sentence)\n",
    "            if sentiment_score['compound'] > 0.5:  # Filter positive sentiment sentences\n",
    "                embedding = model.encode(sentence)\n",
    "                job_embedding = model.encode(job_description)\n",
    "                similarity_score = util.cos_sim(job_embedding, embedding).item()\n",
    "                if similarity_score > 0.5:  # Filter relevant sentences\n",
    "                    relevant_sentences.append(sentence)\n",
    "                    similarity_scores.append(similarity_score)\n",
    "    return relevant_sentences, similarity_scores\n",
    "\n",
    "# Your Google Custom Search API key and cx\n",
    "google_api_key = \"AIzaSyBeKI6179qYKYQlKF0bZ97Lp4l2YfmdYnM\"\n",
    "google_cx = \"f46a1f3417d09439b\"\n",
    "\n",
    "\n",
    "# Query for Google search\n",
    "google_query = \" \".join(noun_phrases) + \" tutorial\"\n",
    "\n",
    "# Fetch Google articles\n",
    "google_articles = fetch_google_articles(google_query, google_api_key, google_cx)\n",
    "# print(\"Google Articles:\", google_articles)\n",
    "\n",
    "# Process Google articles\n",
    "google_relevant_sentences, google_similarity_scores = process_articles(google_articles, job_description)\n",
    "\n",
    "# Print comparison results\n",
    "# Print comparison results\n",
    "print(\"Number of Similarity Scores:\", len(google_similarity_scores))\n",
    "print(\"Google Articles Relevance:\")\n",
    "for i, (sentence, link) in enumerate(google_articles):\n",
    "    print(f\"URL: {link}\")\n",
    "    print(f\"Sentence: {sentence}, Similarity: {google_similarity_scores[i]:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
